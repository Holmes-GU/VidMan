<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation">
  <meta name="keywords" content="imitation learning mobile manipulation, robots ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <h1 style="font-size: 55px; font-weight: bold; text-align: center;">VidMan: Exploiting Implicit Dynamics from Video <br> Diffusion Model for Effective Robot Manipulation  </h1>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="is-size-5 publication-authors" style="padding-top: 10px;">
            <span class="author-block">
              Youpeng Wen<sup>1</sup><sup>&dagger;</sup>,
            </span>
            <span class="author-block">
              Junfan Lin<sup>2</sup><sup>&dagger;</sup>,
            </span>
            <span class="author-block">
              Yi Zhu<sup>3</sup>,
            </span>
            <span class="author-block">
              Jianhua Han<sup>3</sup>,
            </span>
            <span class="author-block">
              Hang Xu<sup>3</sup>,
            </span>
            <span class="author-block">
              Shen Zhao<sup>1</sup><sup>*</sup>,
            </span>
            <span class="author-block">
              Xiaodan Liang <sup>1,2</sup><sup>*</sup>
            </span>
          </div>
          <br>

          <div class="is-size-5 publication-authors">
            <span class="font-size: 20px;"><sup>1</sup>Sun Yat-Sen University,<sup>2</sup>Peng Cheng Laboratory,<sup>3</sup>Huawei Noah's Ark Lab</span>
            <br>
            <span style="font-size: 20px;"><sup>&dagger;</sup>Equal Contribution</span>
            <br>
            <span style="font-size: 20px;"><sup>*</sup>Corresponding author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2411.09153"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span style="font-size: 22px;">arXiv</span>
                </a>
              </span>
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2411.09153"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span style="font-size: 22px;">Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/jirufengyu/VidMan"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span style="font-size: 22px;">Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <!-- <div class="content has-text-justified">
    <div class="columns is-centered has-text-centered">
      <div class="container">
        <video poster="" id="calvin_0" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; position: relative; left: -1%;">
          <source src="static/videos/intro/calvin_0.mp4"
                  type="video/mp4">
        </video>
        <video poster="" id="calvin_1" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; ; position: relative; left: -0.5%;">
          <source src="static/videos/intro/calvin_1.mp4"
                  type="video/mp4">
        </video>
        <video poster="" id="calvin_2" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; position: relative; left: 0.5%;">
          <source src="static/videos/intro/calvin_2.mp4"
                  type="video/mp4">
        </video>
        <video poster="" id="calvin_3" autoplay controls muted loop playsinline width="24%" style="border-radius: 4%; ; position: relative; left: 1%;">
          <source src="static/videos/intro/calvin_3.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
           <div class="item open_the_drawer">
            <video poster="" id="open_the_drawer" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/real_exp/open_the_drawer.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item 3task_distractor">
            <video poster="" id="3task_distractor" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/intro/3task_distractor.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item 3task_unseen">
            <video poster="" id="3task_unseen" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/intro/3task_unseen.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item 3task_normal">
            <video poster="" id="3task_normal" autoplay controls muted loop playsinline height="90%">
              <source src="static/videos/intro/3task_normal.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

  </div> -->

  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            Recent advancements utilizing large-scale video data for learning video generation models demonstrate significant potential in understanding complex physical dynamics. 
            It suggests the feasibility of leveraging diverse robot trajectory data to develop a unified, dynamics-aware model to enhance robot manipulation. 
            However, given the relatively small amount of available robot data, directly fitting data without considering the relationship between visual observations and actions could lead to suboptimal data utilization. 
            To this end, we propose <b>VidMan</b> (<b>Vid</b>eo Diffusion for Robot <b>Man</b>ipulation), a novel framework that employs a two-stage training mechanism inspired by dual-process theory from neuroscience to enhance stability and improve data utilization efficiency. 
            Specifically, in the first stage, VidMan is pre-trained on the Open X-Embodiment dataset (OXE) for predicting future visual trajectories in a video denoising diffusion manner, enabling the model to develop a long horizontal awareness of the environment's dynamics. 
            In the second stage, a flexible yet effective layer-wise self-attention adapter is introduced to transform VidMan into an efficient inverse dynamics model that predicts action modulated by the implicit dynamics knowledge via parameter sharing. 
            Our VidMan framework outperforms state-of-the-art baseline model GR-1 on the CALVIN benchmark, achieving a 11.7% relative improvement, and demonstrates over 9% precision gains on the OXE small-scale dataset. 
            These results provide compelling evidence that world models can significantly enhance the precision of robot action prediction. <br>
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Method</h2>
        <br>
        <div class="columns is-centered has-text-centered">
          <img id="multi_methods" width="80%" src="static/images/teaserfig.jpg"/>
        </div>
        <!-- <video poster="" id="overview" autoplay muted loop playsinline width="70%">
            <source src="static/videos/intro/overview.mp4"
                    type="video/mp4">
        </video> -->
        <div class="content is-centered has-text-justified">
          <p  style="font-size: 24px;">
            VidMan's two-stage training paradigm mirrors dual process theory: its first stage (like
 System 2) pre-trains on understanding environment dynamics through video diffusion, forming a
 foundation for accurate action prediction, while its second stage (like System 1) was adapted from
 the first stage to leverage the learned dynamics knowledge for rapid, low-level action inference.
          </p>
        </div>
        <br>
        <div class="columns is-centered has-text-centered">
            <img id="pipeline" width="80%" src="static/images/mainfig.jpg"/>
        </div>
        <div class="content is-centered has-text-justified">
          <p  style="font-size: 24px;">
            VidMan employs a dual-stage training strategy: in the first
 stage, the Dynamics-aware Visionary Stage, we enable the model to forecast and imagine potential
 future trajectories based on historical observations, leveraging the multi-frame prediction capability
 of the video diffusion model. Through this stage, the model is optimized to understand the dynamics
 of the environment. In the second stage, the Dynamics-modulated Action Stage, we introduce
 a lightweight layer-wise adapter to seamlessly integrate the visionary predictive stage with fast,
 adaptive action prediction. This approach decouples the knowledge of the world and embodiment
 into distinct processes while ensuring seamless integration through the training and utilization of
 shared parameters.
          </p>
        </div>
      </div>
    </div>
</section>


<!-- <section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Real Robot Experiments</h2> -->
        <!-- <img id="robot_result" width="80%" src="static/images/real_robot.png"> -->
        <!-- <div class="content has-text-justified">
          <br>
          <p style="font-size: 24px;">
          We conducted real-world experiments, where we set up three tasks: 
          (i) "Pick up": pick up the correct object from the table. 
          (ii) "Put on": Pick up the object and place it on the correct color block. 
          (iii) "Push to": Push the object to the correct color block.  
          We collected 400, 200, and 200 sets of demonstrations respectively. 
          PIVOT-R achieved a 6% improvement over the best baseline.
          </p>
          <div class="columns is-centered has-text-centered">
            <div class="container" style="display: flex; justify-content: center; flex-wrap: nowrap;">
              <div style="flex: 1; margin-right: 2%;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -1%;">
                  <source src="static/videos/push_coffee_to_pink_block.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Push coffee to pink block.</p>
              </div>
              <div style="flex: 1; margin-right: 2%;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -0.5%;">
                  <source src="static/videos/pick_up_the_juice_in the_front_row.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Pick up the juice in the front row.</p>
              </div>
              <div style="flex: 1;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                  <source src="static/videos/pick_up_starbucks_and_put_it_on_yellow_block.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Pick up starbucks and put it on yellow block.</p>
              </div>
            </div>
          </div>
    </div>
</section> -->

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">CALVIN Benchmark Experiments</h2>
        <!-- <img id="calvin_env" width="70%" src="static/images/calvin_env.png" style="display: block; margin-left: auto; margin-right: auto"> -->
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            <br>
             We choose <a href="https://arxiv.org/abs/2112.03227"><b>CALVIN</b></a>, an open-source benchmark to learn long term tasks, 
             as our experimental platform, and use the corresponding data as demonstration data for imitation learning. 
 CALVIN provides 24 hours of unstructured tele-operated play data, with 1%
 annotated with language descriptions. Each instruction chain consists of five sequential language
 instructions for execution. Evaluation follows a zero-shot generalization setup, training models on
 environments A, B, and C and testing on D. Performance metrics include success rates and average
 completion of sequential tasks. Results are shown in the below.
          </p>
          <div class="columns is-centered">
             <img id="table1" width="70%" src="static/images/calvintable.jpg" style="display: block; margin-left: auto; margin-right: auto">
          </div>
          <div class="container" style="display: flex; justify-content: space-between; flex-wrap: wrap;">
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <!-- <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -1%;">
                <source src="static/videos/simulation/9-2-lift_blue_block_slider-succ.gif" type="video/gif">
              </video> -->
              <img src="static/videos/simulation/9-2-lift_blue_block_slider-succ.gif">
              <p style="font-size: 20px;">Lift the blue block in the slider.</p>
            </div>
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <!-- <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -0.5%;">
                <source src="static/videos/simulation/10-1-place_in_drawer-succ.gif" type="video/gif">
              </video> -->
              <img src="static/videos/simulation/10-1-place_in_drawer-succ.gif">
              <p style="font-size: 20px;">Place the red block in the drawer.</p>
            </div>
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <!-- <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                <source src="static/videos/simulation/13-0-open_drawer-succ.gif" type="video/gif">
              </video> -->
              <img src="static/videos/simulation/13-0-open_drawer-succ.gif">
              <p style="font-size: 20px;">Open the drawer.</p>
            </div>
            <div style="flex: 1; margin: 0.5%; text-align: center;">
              <!-- <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                <source src="static/videos/simulation/15-0-turn_off_led-succ.gif" type="video/gif">
              </video> -->
              <img src="static/videos/simulation/15-0-turn_off_led-succ.gif">
              <p style="font-size: 20px;">Turn off the led.</p>
            </div>
          </div>
          <br>
          <div class="columns has-text-justified">
          <p style="font-size: 24px;">
            <br>
            We also report offline metrics, including the average of xyz accuracy and
 euler angle accuracy (Avg xyz ang) and MSE for end-to-end action prediction on Bridge, Taco
 Play, Cable Routing and Autolab UR5, which are presented in OXE. Following
 Octo, we use continuous action space. XYZ accuracy measures the precision of the robot's
 predicted 3D position (X, Y, Z coordinates) compared to the ground truth values during evaluation.
 Euler angle accuracy measures the precision of the robot's predicted orientation angles (rotations
 around X, Y, and Z axes) compared to the ground truth values during offline evaluation. Specifically,
 XYZaccuracy refers to whether we predicted the XYZ delta within 0.5 radians and 50% of the norm
 while in motion. Euler angle accuracy indicates whether we predicted the rotation delta within 0.5
 radians during movement. Additionally, we reported the mean squared error (MSE) which reflects
 how well each model predicts the actions. We present the offline evaluation results and video prediction results on OXE below. 
          </p>
          <br>
          </div>
          <div class="columns">
            <img id="table2" width="70%" src="static/images/oxetable.jpg" style="display: block; margin-left: auto; margin-right: auto">
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="container" style="display: flex; justify-content: center; flex-wrap: nowrap;">
              <div style="flex: 1; margin-right: 2%;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -1%;">
                  <source src="static/videos/predict_videos/vd0.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Move red pepper to above green towel.</p>
              </div>
              <div style="flex: 1; margin-right: 2%;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -0.5%;">
                  <source src="static/videos/predict_videos/vd2.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Fold the cloth from top left to bottom right.</p>
              </div>
              <div style="flex: 1; margin-right: 2%;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: -0.5%;">
                  <source src="static/videos/predict_videos/vd28.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Unfold the cloth from top left to bottom right.</p>
              </div>
              <div style="flex: 1;">
                <video poster="" autoplay controls muted loop playsinline width="100%" style="border-radius: 4%; position: relative; left: 0.5%;">
                  <source src="static/videos/predict_videos/vd5.mp4" type="video/mp4">
                </video>
                <p style="font-size: 20px;">Put carrot in pot cardboard fence.</p>
              </div>
            </div>
          </div>
      </div>
    </div>
</section>

<!-- <section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Ablation Studies</h2>
        <div class="content has-text-justified">
            <div class="column has-text-left" style="padding-top: 25px;">
              <p style="font-size: 24px;">
                GR-1 features video prediction and large-scale pretraining on video prediction.
                We perform ablation studies to study how these two factors influence the performance.
                GR-1 outperforms the variant without pre-training and the variant without pre-training and video prediction in all experiments. 
                We hypothesize that this is because the large-scale video pre-training helps GR-1 learn an accurate video prediction model which helps the robot understand what shall happen in future steps given the language instruction and previous observations. 
                And this information acts as a strong signpost for the robot to generate pertinent actions for rolling out trajectories. 
                Without pre-training, the video prediction of GR-1 w/o Video Pre-training may not be as robust.
                <br><br>
              </p>
            </div>
            <div class="columns">
              <div class="column has-text-left">
                  <img src="static/images/ablation_ABCD_D.png" class="interpolation-image"
                  alt="" style="display: block; margin-left: auto; margin-right: auto"/>
              </div>
              <div class="column has-text-left">
                  <img src="static/images/ablation_ABC_D.png" class="interpolation-image"
                alt="" style="display: block; margin-left: auto; margin-right: auto"/>
              </div>
              <div class="column has-text-left">
                <img src="static/images/ablation_10_percent_data.png" class="interpolation-image"
              alt="" style="display: block; margin-left: auto; margin-right: auto"/>
            </div>
            </div>
            <div class="column has-text-left" style="padding-top: 25px;">
              <p style="font-size: 24px;">
                We probe into GR-1 to investigate its video prediction performance on CALVIN and real robot data.
                Results are shown in the below figure in which the images in the green boxes are the ground-truth images and those in the blue boxes are the predicted images.
                GR-1 is able to reconstruct future frames correctly on both CALVIN data and real robot data, although some details (e.g. occluded objects) are missing. 
                This video prediction signal can serve as a strong guide for action predictions.
                More results can be found in the paper.
              </p>
            </div>
            <img id="robot_result" width="65%" style="display: block;margin-left: auto; margin-right: auto;" src="static/images/fwd_pred.png">
        </div> 
      </div>
    </div>
</section> -->

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Conclusions</h2>
        <div class="content has-text-justified">
          <p style="font-size: 24px;">
            We propose VidMan, a novel framework utilizing video diffusion models for robot
 imitation learning, which addresses the limitations of current GPT-style paradigms in real-time
 applications. By combining a Dynamics-aware Visionary Stage, which develops a deep understanding
 of environment dynamics through pre-training on the Open X-Embodiment dataset, with a Dynamics
modulated Action Stage that efficiently integrates this knowledge into action prediction, VidMan
 achieves both high precision and computational efficiency. This two-stage approach, ensures robust
 and rapid action generation, significantly improving performance on benchmarks like CALVIN and
 the OXE dataset. In the future, we will expand VidMan to be able to perceive more dimensions of
 information.
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title" style="text-align: left;">BibTeX</h2>
        <div class="content has-text-justified">
          <pre><code style="font-size: 24px;">@inproceedings{wenvidman,
               title={VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation},
               author={Wen, Youpeng and Lin, Junfan and Zhu, Yi and Han, Jianhua and Xu, Hang and Zhao, Shen and Liang, Xiaodan},
               booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems}
               }</code></pre>
        </div>
      </div>
    </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
        <div class="content">
          <p style="font-size: 24px;">
            The website template was adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
  </div>
</footer>

</body>
</html>
